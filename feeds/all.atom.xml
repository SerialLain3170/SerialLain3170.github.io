<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>lain</title><link href="/" rel="alternate"></link><link href="/feeds/all.atom.xml" rel="self"></link><id>/</id><updated>2024-01-08T00:00:00+09:00</updated><entry><title>Links</title><link href="/2-links.html" rel="alternate"></link><published>2024-01-08T00:00:00+09:00</published><updated>2024-01-08T00:00:00+09:00</updated><author><name>lento</name></author><id>tag:None,2024-01-08:/2-links.html</id><content type="html">&lt;h2&gt;Summary of Links&lt;/h2&gt;
&lt;h3&gt;Overview&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://seriallain3170.github.io/about.html#about"&gt;Self-introduction (en)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://scrapbox.io/serialexperimentscrie/%E8%BF%91%E6%B3%81"&gt;Recent days (ja)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://scrapbox.io/serialexperimentscrie/"&gt;Self-reflection (ja)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Research and Development&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://scholar.google.com/citations?user=tzKL43UAAAAJ&amp;amp;hl=en"&gt;Google scholar&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/SerialLain3170"&gt;Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/@crosssceneofwindff"&gt;Tech blog (ja)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://seriallain3170.github.io/"&gt;Tech blog (en)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;SNS&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://twitter.com/NieA7_3170"&gt;Twitter (ja)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://twitter.com/Crie_functional"&gt;Twitter (en)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.linkedin.com/in/so-hasegawa-5aa509169/"&gt;LinkedIn&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Hobby&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://bookmeter.com/users/1378090"&gt;Bookmeter (ja)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://soundcloud.com/lento-3"&gt;Soundcloud&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.flickr.com/photos/197623303@N08/"&gt;flickr&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="links"></category><category term="links"></category></entry><entry><title>AwesomeAnimeResearch: Past, Present, and Future</title><link href="/1-awesomeanimeresearch.html" rel="alternate"></link><published>2022-11-24T00:00:00+09:00</published><updated>2022-11-24T00:00:00+09:00</updated><author><name>lento</name></author><id>tag:None,2022-11-24:/1-awesomeanimeresearch.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I have created and managed a repository, named &lt;a href="https://github.com/SerialLain3170/AwesomeAnimeResearch"&gt;AwesomeAnimeResearch&lt;/a&gt; for more than two years (the first commit was at 04/27/2020). Thanks to contributors and issue builders, we have collected more than 250 papers related to anime, illustration, and comic. I would like to explain motivation to create …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I have created and managed a repository, named &lt;a href="https://github.com/SerialLain3170/AwesomeAnimeResearch"&gt;AwesomeAnimeResearch&lt;/a&gt; for more than two years (the first commit was at 04/27/2020). Thanks to contributors and issue builders, we have collected more than 250 papers related to anime, illustration, and comic. I would like to explain motivation to create the repository, design, present challenges, and future plans.&lt;/p&gt;
&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Until I created the repository, I used to take a look at &lt;a href="https://github.com/shubhampachori12110095/DeepLearningAnimePapers"&gt;DeepLearningAnimePapers&lt;/a&gt; to investigate papers related to anime. Since it had stopped updates in 2018, I had possessed a desire to create awesome-list for anime papers that contains a massive and exciting collection. Therefore, &lt;a href="https://twitter.com/NieA7_3170/status/1258700433356283910?s=20&amp;amp;t=PrGYp5veSVDrUamtQqjMCg"&gt;I started to manage AwesomeAnimeResearch&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Design&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;In the first place, what is the definition of "papers related to anime, illustration, and comic"? I have defined it as the published or preprint papers that explicitly include applications for these fields. For example, if the paper is about image generation and includes the generated figures of not only human faces but also anime characters, the paper clearly shows potentials for anime applications. Of course, since the definition is from all my perspectives, I am willing to discuss the appropriate definition.&lt;/li&gt;
&lt;li&gt;There are 2 categories of resources in AwesomeAnimeResearch: paper and project. Papers are the papers in accordance with the definition that I stated. Projects are not the papers but they are tech reports or github repositories that have contributed to promoting machine learning research related to anime (e.g. &lt;a href="https://make.girls.moe/#/"&gt;makegirlsmoe&lt;/a&gt; or &lt;a href="https://github.com/lllyasviel/style2paints"&gt;style2paints&lt;/a&gt;). &lt;/li&gt;
&lt;li&gt;I have set 21 categories like below based on my perspective, and four categories have their own subcategories.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;- Dataset
- Image Generation
  |- Generation
  |- Few-shot
  |- Interpretability
  |- Montage
- Image-to-Image Translation
  |- Face2anime
  |- Selfie2anime
  |- Photo2anime
  |- Sketch2anime
  |- Photo2manga
  |- Anime2costume
  |- Style transfer
  |- Author style transfer
- Automatic Line Art Colorization
  |- NoHint
  |- Atari
  |- Reference
  |- Tag
  |- Video
- Automatic Character Lighting
- Automatic Illustration Editing
- Automatic Sketch Editing
- Automatic Animation Inbetweening
- Automatic Image Enhancement
- Character Animating
- Manga Application
  |- Generation
  |- Restoration
  |- Inpainting
  |- Text detection
  |- Landmark detection
  |- Segmentation
  |- Translation
  |- Depth estimation
  |- Vectorization
  |- Re-identification
- Representation Learning
- Pose Estimation
- Image Retrieval
- Visual Correspondence
- Character Recognition
- 3D Character Creation
- Robotics
- Speech Synthesis
- Adult Content Detection
- Survey
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;When it comes to the papers, each record has three or four fields depending on the existence of subcategories.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;Subcategory&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;optional&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;:&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;category&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;does&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;not&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;have&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;subcategories&lt;/span&gt;,&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;this&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;field&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;does&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;not&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;exist&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;Paper&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;required&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;:&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;The&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;link&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;and&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;name&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;of&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;paper&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;Conference&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;optional&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;:&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;The&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;abbreviated&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;form&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;of&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;conference&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;name&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;including&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;year&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;Links&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;optional&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;:&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;Webpages&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;describing&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;paper&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;aside&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;published&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;paper&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;e&lt;/span&gt;.&lt;span class="nv"&gt;g&lt;/span&gt;.&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;a&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;Github&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;repository&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;or&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;a&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;specific&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;webpage&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;.&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;Multiple&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;sources&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;are&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;OK&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;As for the projects, each record has the link and the name of the project. If the project is about the tech report and the webpage, the title is treated as the name. If the project is about the github repository, a combination of the username and the name of the repository corresponds to the name.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Present Challenges&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Diversity&lt;ul&gt;
&lt;li&gt;There are two aspects about the diversity: number of papers and uniqueness.&lt;ul&gt;
&lt;li&gt;Number of papers: I have noticed that the current version of AwesomeAnimeResearch lacks papers among several categories. Some categories include abundant papers, and others do not. I have to admit that the lack comes from my interests. Although several issue builders and contributers have mitigated the problem, there seems still be imbalanced with respect to the number of papers. Moreover, the categorization that I showed in the design section is not expected for researchers working on speech processing and natural language processing because the current categories lack papers related to the areas.&lt;/li&gt;
&lt;li&gt;Uniqueness: Papers among some categories would be overabundant because several pairs of the papers in one category mostly resemble each other. The truncation would be imperative to enable users to access to the desired paper faster.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Completeness&lt;ul&gt;
&lt;li&gt;A lot of dead links for the paper and blanks in the conference name exist. Especially as for the former problem, it significantly decreases the accessibility of users.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Future Plans&lt;/h2&gt;
&lt;p&gt;I will continue to add papers and projects to the repository via a search in reddit and google scholar. I also always accept issues and PR from others. Especially the latter leads to mitigating the problems of diversity. On the other hand, I am thinking about creating the script to search papers related to each category. This script would not only solve the problem of dead links and blanks but also enhance the objectiveness in searching the papers. If you have questions, corrections, or suggestions, I am willing to welcome and discuss them via Github Issue.&lt;/p&gt;</content><category term="cs"></category><category term="cs"></category></entry><entry><title>About Me</title><link href="/about.html" rel="alternate"></link><published>2022-06-19T00:00:00+09:00</published><updated>2023-03-31T00:00:00+09:00</updated><author><name>lento</name></author><id>tag:None,2022-06-19:/about.html</id><summary type="html">&lt;h2&gt;Personal Information&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Name: So Hasegawa&lt;/li&gt;
&lt;li&gt;Age: 29&lt;/li&gt;
&lt;li&gt;Mail: crosssceneofwindff |at| gmail |dot| com&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Publications and Conferences&lt;/h2&gt;
&lt;h3&gt;Peer reviewed&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://openreview.net/forum?id=OJ8aSjCaMNK"&gt;Multi-Rate VAE: Train Once, Get the Full Rate-Distortion Curve&lt;/a&gt;&lt;br&gt;
  J. Bae, M. R. Zhang, M. Ruan, E. Wang, &lt;strong&gt;S. Hasegawa&lt;/strong&gt;, J. Ba, R. Grosse&lt;br&gt;
  In International Conference on Learning Representations (ICLR), 2023 …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;h2&gt;Personal Information&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Name: So Hasegawa&lt;/li&gt;
&lt;li&gt;Age: 29&lt;/li&gt;
&lt;li&gt;Mail: crosssceneofwindff |at| gmail |dot| com&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Publications and Conferences&lt;/h2&gt;
&lt;h3&gt;Peer reviewed&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://openreview.net/forum?id=OJ8aSjCaMNK"&gt;Multi-Rate VAE: Train Once, Get the Full Rate-Distortion Curve&lt;/a&gt;&lt;br&gt;
  J. Bae, M. R. Zhang, M. Ruan, E. Wang, &lt;strong&gt;S. Hasegawa&lt;/strong&gt;, J. Ba, R. Grosse&lt;br&gt;
  In International Conference on Learning Representations (ICLR), 2023&lt;/li&gt;
&lt;li&gt;&lt;a href="https://openaccess.thecvf.com/content/WACV2023/html/Hasegawa_Improving_Predicate_Representation_in_Scene_Graph_Generation_by_Self-Supervised_Learning_WACV_2023_paper.html"&gt;Improving Predicate Representation in Scene Graph Generation by Self-Supervised Learning&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;S. Hasegawa&lt;/strong&gt;, M. Hiromoto, A. Nakagawa, Y. Umeda&lt;br&gt;
  In IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2023&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Non-peer reviewed&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://onepetro.org/SPEADIP/proceedings-abstract/20ADIP/3-20ADIP/D031S080R004/452202"&gt;Facilitating the Identification of the Nannofossil Species in Cretaceous of Abu Dhabi Using Artificial Intelligence&lt;/a&gt;&lt;br&gt;
  H. Tamamura, M. Yamanaka, S. Chiyonobu, G. Yamada, &lt;strong&gt;S. Hasegawa&lt;/strong&gt;, Y. Totake, T. Nanjo&lt;br&gt;
  In Abu Dhabi International Petroleum Exhibition &amp;amp; Conference, 2020&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.jstage.jst.go.jp/article/jsapmeeting/2017.2/0/2017.2_848/_article/-char/ja/"&gt;Analysis of light absorption characteristics in very-thin single-crystalline silicon solar cells with photonic crystals&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;S. Hasegawa&lt;/strong&gt;, K. Ishizaki, Y. Tanaka, and S. Noda&lt;br&gt;
  In 78th Japan Society of Applied Physics (JSAP) Autumn Meeting, 2017&lt;/li&gt;
&lt;li&gt;&lt;a href="https://confit.atlas.jp/guide/event/jsap2016s/subject/22a-S621-3/advanced"&gt;Numerical analysis of μc-Si solar cells with photonic crystals formed on top surface -Introduction of asymmetric structure-&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;S. Hasegawa&lt;/strong&gt;, K. Ishizaki, Y. Tanaka, A. Motohira, Y. Kawamoto, M. De Zoysa, S. Fujita, and S. Noda&lt;br&gt;
  In 63th Japan Society of Applied Physics (JSAP) Spring Meeting, 2016&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Invited Lectures and Talks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.slideshare.net/SouHasegawa/generative-adversarial-networks-186237021"&gt;Generative Adversarial Networksの基礎と応用について (Basics and Applications of Generative Adversarial Network)&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;S. Hasegawa&lt;/strong&gt;&lt;br&gt;
  Invited lecture at Tohoku University, 2019&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.slideshare.net/SouHasegawa/ss-189697036"&gt;データに寄りそう着色の作法 (Basics of Line Art Colorization)&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;S. Hasegawa&lt;/strong&gt;&lt;br&gt;
  Talk at &lt;a href="https://connpass.com/event/149734/"&gt;創作＋機械学習LT会&lt;/a&gt;, 2019&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Education&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Apr 2016 ~ Mar 2018: MEng in Electronics at Kyoto University&lt;ul&gt;
&lt;li&gt;Thesis: Fabrication and Evaluation of Thin Single-Crystalline Silicon Solar Cells with Photonic Crystals&lt;/li&gt;
&lt;li&gt;Supervisor: Susumu Noda&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Apr 2012 ~ Mar 2016: BEng in Electrical and Electronic Engineering at Kyoto University&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Work Experience&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Oct 2022 ~ : machine learning researcher at Fujitsu Research of America&lt;ul&gt;
&lt;li&gt;My research is focused on AutoML&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Aug 2021 ~ Sep 2022: machine learning researcher at Fujitsu Research&lt;ul&gt;
&lt;li&gt;My research was focused on scene graph generation, self-supervised learning, and generative models&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sep 2019 - Jan 2020: research assistant at SYMBOL&lt;ul&gt;
&lt;li&gt;My work was focused on developing deep learning-based solutions to extract essential information from 3D data (point cloud, mesh, multi-view)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Apr 2018 ~ Jul 2021: software engineer at Fujitsu&lt;ul&gt;
&lt;li&gt;My work was focused on providing deep learning-based solutions with the customers and developing software products using speech processing and computer vision technologies&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Personal Projects&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Project name&lt;/th&gt;
&lt;th&gt;Descrption&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/SerialLain3170/adeleine"&gt;adeleine&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Deep learning-based implementations of line art colorization with hints and without hints. Userhintv2 model in the repository is used in &lt;a href="https://experiments.withgoogle.com/giga-manga"&gt;Giga Manga&lt;/a&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/SerialLain3170/senju"&gt;senju&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Several deep-learning based implementations pertaining to anime. The repository also includes GUI application consisting of Go server and Python modules connected via gRPC.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/SerialLain3170/accela"&gt;accela&lt;/a&gt; and &lt;a href="https://github.com/SerialLain3170/cyberia"&gt;cyberia&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Music digging tools. accela is implemented in TypeScript, and cyberia is done in Python.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href="https://github.com/SerialLain3170/AwesomeAnimeResearch"&gt;AwesomeAnimeResearch&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;A massive collection of machine learning papers and projects related to anime&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;Skills&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Computer Science&lt;ul&gt;
&lt;li&gt;Machine Learning, Deep learning&lt;/li&gt;
&lt;li&gt;Signal Processing: Computer vision, Speech processing&lt;/li&gt;
&lt;li&gt;Low-level Programming: Emulators of NES, GB, CGB&lt;/li&gt;
&lt;li&gt;Programming Language: Python, Go, C&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Software Development&lt;ul&gt;
&lt;li&gt;Team development&lt;/li&gt;
&lt;li&gt;OS: Linux(Ubuntu, CentOS), MacOS, Windows&lt;/li&gt;
&lt;li&gt;Tools: Docker, Docker-compose, Git, Gitlab, Github, MySQL, PostgreSQL, MongoDB, Nodejs, gRPC, Nginx, RabbitMQ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Language&lt;ul&gt;
&lt;li&gt;Japanese: native&lt;/li&gt;
&lt;li&gt;English: advanced (TOEFL: 96, IELTS: 7.0, GRE: V153/Q170/AW3.5)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Interests&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Programming (visit &lt;a href="https://github.com/SerialLain3170"&gt;Github&lt;/a&gt; and &lt;a href="https://medium.com/@crosssceneofwindff"&gt;Medium&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Visual arts: watching all kinds of arts, visiting art museums, and learning art history&lt;/li&gt;
&lt;li&gt;Books: science (computer science, biology, geology), art, history, finance (visit &lt;a href="https://bookmeter.com/users/1378090"&gt;Bookmeter&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Music: listening, playing the piano, composing (visit &lt;a href="https://soundcloud.com/lento-3"&gt;Soundcloud&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Travel and enjoy nature (visit &lt;a href="https://www.flickr.com/photos/197623303@N08/"&gt;flickr&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;</content><category term="about"></category><category term="about"></category></entry></feed>