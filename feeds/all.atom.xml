<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>lain</title><link href="/" rel="alternate"></link><link href="/feeds/all.atom.xml" rel="self"></link><id>/</id><updated>2022-11-29T00:00:00+09:00</updated><entry><title>Reading Memo: De novo protein design by deep network hallucination</title><link href="/2-hallucination.html" rel="alternate"></link><published>2022-11-25T00:00:00+09:00</published><updated>2022-11-25T00:00:00+09:00</updated><author><name>lento</name></author><id>tag:None,2022-11-25:/2-hallucination.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This paper proposes a novel deep learning-based approach to predict new 3D protein structures with new amino acid sequences using hallucination technique. Although deep learning has contributed to predicting new 3D structures from existing amino acid sequences and new sequences from 3D structures, predicting 3D protein structures with new …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This paper proposes a novel deep learning-based approach to predict new 3D protein structures with new amino acid sequences using hallucination technique. Although deep learning has contributed to predicting new 3D structures from existing amino acid sequences and new sequences from 3D structures, predicting 3D protein structures with new sequences has not been tackled. I would like to explain the paper by focusing on algorithm parts and technical limitations.&lt;/p&gt;
&lt;h2&gt;DeepDream&lt;/h2&gt;
&lt;p&gt;Before diving into the hallucination technique, I have to elucidate DeepDream because the paper refers to the algorithm. DeepDream is the deep learning-based algorithm that produces much bizarre images from input images by emphasizing the particular parts of the input images. Given the pre-trained model trained with a dataset including a lot of dog images, the model tries to detect dogs in images regardless of existence of dogs. If the input image is about a sky and contains clouds that look like dogs, the model taking the image as the input tends to focus on these regions, and the patterns of activated neurons to detect dogs are observed in the model. After setting these patterns as "loss" and backpropagating the loss to the inputs, the input image is modified so that the model succeeds in detecting dogs with high accuracy. The modification leads to the image that has dog-like clouds.&lt;/p&gt;
&lt;h2&gt;Algorithm&lt;/h2&gt;
&lt;p&gt;Let us dive into the algorithm of the hallucination technique for designing 3D protein structures with new amino acid sequences. &lt;/p&gt;</content><category term="biology"></category><category term="biology"></category></entry><entry><title>AwesomeAnimeResearch: Past, Present, and Future</title><link href="/1-awesomeanimeresearch.html" rel="alternate"></link><published>2022-11-24T00:00:00+09:00</published><updated>2022-11-24T00:00:00+09:00</updated><author><name>lento</name></author><id>tag:None,2022-11-24:/1-awesomeanimeresearch.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I have created and managed a repository, named &lt;a href="https://github.com/SerialLain3170/AwesomeAnimeResearch"&gt;AwesomeAnimeResearch&lt;/a&gt; for more than two years (the first commit was at 04/27/2020). Thanks to contributors and issue builders, we have collected more than 250 papers related to anime, illustration, and comic. I would like to explain motivation to create …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I have created and managed a repository, named &lt;a href="https://github.com/SerialLain3170/AwesomeAnimeResearch"&gt;AwesomeAnimeResearch&lt;/a&gt; for more than two years (the first commit was at 04/27/2020). Thanks to contributors and issue builders, we have collected more than 250 papers related to anime, illustration, and comic. I would like to explain motivation to create the repository, design, present challenges, and future plans.&lt;/p&gt;
&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Until I created the repository, I used to take a look at &lt;a href="https://github.com/shubhampachori12110095/DeepLearningAnimePapers"&gt;DeepLearningAnimePapers&lt;/a&gt; to investigate papers related to anime. Since it had stopped updates in 2018, I had possessed a desire to create awesome-list for anime papers that contains a massive and exciting collection. Therefore, &lt;a href="https://twitter.com/NieA7_3170/status/1258700433356283910?s=20&amp;amp;t=PrGYp5veSVDrUamtQqjMCg"&gt;I started to manage AwesomeAnimeResearch&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Design&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;In the first place, what is the definition of "papers related to anime, illustration, and comic"? I have defined it as the published or preprint papers that explicitly include applications for these fields. For example, if the paper is about image generation and includes the generated figures of not only human faces but also anime characters, the paper clearly shows potentials for anime applications. Of course, since the definition is from all my perspectives, I am willing to discuss the appropriate definition.&lt;/li&gt;
&lt;li&gt;There are 2 categories of resources in AwesomeAnimeResearch: paper and project. Papers are the papers in accordance with the definition that I stated. Projects are not the papers but they are tech reports or github repositories that have contributed to promoting machine learning research related to anime (e.g. &lt;a href="https://make.girls.moe/#/"&gt;makegirlsmoe&lt;/a&gt; or &lt;a href="https://github.com/lllyasviel/style2paints"&gt;style2paints&lt;/a&gt;). &lt;/li&gt;
&lt;li&gt;I have set 21 categories like below based on my perspective, and four categories have their own subcategories.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;- Dataset
- Image Generation
  |- Generation
  |- Few-shot
  |- Interpretability
  |- Montage
- Image-to-Image Translation
  |- Face2anime
  |- Selfie2anime
  |- Photo2anime
  |- Sketch2anime
  |- Photo2manga
  |- Anime2costume
  |- Style transfer
  |- Author style transfer
- Automatic Line Art Colorization
  |- NoHint
  |- Atari
  |- Reference
  |- Tag
  |- Video
- Automatic Character Lighting
- Automatic Illustration Editing
- Automatic Sketch Editing
- Automatic Animation Inbetweening
- Automatic Image Enhancement
- Character Animating
- Manga Application
  |- Generation
  |- Restoration
  |- Inpainting
  |- Text detection
  |- Landmark detection
  |- Segmentation
  |- Translation
  |- Depth estimation
  |- Vectorization
  |- Re-identification
- Representation Learning
- Pose Estimation
- Image Retrieval
- Visual Correspondence
- Character Recognition
- 3D Character Creation
- Robotics
- Speech Synthesis
- Adult Content Detection
- Survey
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;When it comes to the papers, each record has three or four fields depending on the existence of subcategories.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;Subcategory&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;optional&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;:&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;category&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;does&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;not&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;have&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;subcategories&lt;/span&gt;,&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;this&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;field&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;does&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;not&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;exist&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;Paper&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;required&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;:&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;The&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;link&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;and&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;name&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;of&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;paper&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;Conference&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;optional&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;:&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;The&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;abbreviated&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;form&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;of&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;conference&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;name&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;including&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;year&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;Links&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;optional&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;:&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;Webpages&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;describing&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;paper&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;aside&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;the&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;published&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;paper&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="ss"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;e&lt;/span&gt;.&lt;span class="nv"&gt;g&lt;/span&gt;.&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;a&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;Github&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;repository&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;or&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;a&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;specific&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;webpage&lt;/span&gt;&lt;span class="ss"&gt;)&lt;/span&gt;.&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;Multiple&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;sources&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;are&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;OK&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;As for the projects, each record has the link and the name of the project. If the project is about the tech report and the webpage, the title is treated as the name. If the project is about the github repository, a combination of the username and the name of the repository corresponds to the name.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Present Challenges&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Diversity&lt;ul&gt;
&lt;li&gt;There are two aspects about the diversity: number of papers and uniqueness.&lt;ul&gt;
&lt;li&gt;Number of papers: I have noticed that the current version of AwesomeAnimeResearch lacks papers among several categories. Some categories include abundant papers, and others do not. I have to admit that the lack comes from my interests. Although several issue builders and contributers have mitigated the problem, there seems still be imbalanced with respect to the number of papers. Moreover, the categorization that I showed in the design section is not expected for researchers working on speech processing and natural language processing because the current categories lack papers related to the areas.&lt;/li&gt;
&lt;li&gt;Uniqueness: Papers among some categories would be overabundant because several pairs of the papers in one category mostly resemble each other. The truncation would be imperative to enable users to access to the desired paper faster.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Completeness&lt;ul&gt;
&lt;li&gt;A lot of dead links for the paper and blanks in the conference name exist. Especially as for the former problem, it significantly decreases the accessibility of users.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Future Plans&lt;/h2&gt;
&lt;p&gt;I will continue to add papers and projects to the repository via a search in reddit and google scholar. I also always accept issues and PR from others. Especially the latter leads to mitigating the problems of diversity. On the other hand, I am thinking about creating the script to search papers related to each category. This script would not only solve the problem of dead links and blanks but also enhance the objectiveness in searching the papers. If you have questions, corrections, or suggestions, I am willing to welcome and discuss them via Github Issue.&lt;/p&gt;</content><category term="cs"></category><category term="cs"></category></entry><entry><title>About Me</title><link href="/about.html" rel="alternate"></link><published>2022-06-19T00:00:00+09:00</published><updated>2022-11-29T00:00:00+09:00</updated><author><name>lento</name></author><id>tag:None,2022-06-19:/about.html</id><summary type="html">&lt;h2&gt;Personal Information&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Name: So Hasegawa&lt;/li&gt;
&lt;li&gt;Age: 28&lt;/li&gt;
&lt;li&gt;Mail: crosssceneofwindff |at| gmail.com&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Publications and Conferences&lt;/h2&gt;
&lt;h3&gt;Peer reviewed&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://openreview.net/forum?id=OJ8aSjCaMNK"&gt;Multi-Rate VAE: Train Once, Get the Full Rate-Distortion Curve&lt;/a&gt;&lt;br&gt;
  J. Bae, M. R. Zhang, M. Ruan, E. Wang, &lt;strong&gt;S. Hasegawa&lt;/strong&gt;, J. Ba, R. Grosse&lt;br&gt;
  In International Conference on Learning Representations (ICLR), 2023&lt;/li&gt;
&lt;li&gt;&lt;a href="https://openaccess.thecvf.com/content/WACV2023/html/Hasegawa_Improving_Predicate_Representation_in_Scene_Graph_Generation_by_Self-Supervised_Learning_WACV_2023_paper.html"&gt;Improving …&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;h2&gt;Personal Information&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Name: So Hasegawa&lt;/li&gt;
&lt;li&gt;Age: 28&lt;/li&gt;
&lt;li&gt;Mail: crosssceneofwindff |at| gmail.com&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Publications and Conferences&lt;/h2&gt;
&lt;h3&gt;Peer reviewed&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://openreview.net/forum?id=OJ8aSjCaMNK"&gt;Multi-Rate VAE: Train Once, Get the Full Rate-Distortion Curve&lt;/a&gt;&lt;br&gt;
  J. Bae, M. R. Zhang, M. Ruan, E. Wang, &lt;strong&gt;S. Hasegawa&lt;/strong&gt;, J. Ba, R. Grosse&lt;br&gt;
  In International Conference on Learning Representations (ICLR), 2023&lt;/li&gt;
&lt;li&gt;&lt;a href="https://openaccess.thecvf.com/content/WACV2023/html/Hasegawa_Improving_Predicate_Representation_in_Scene_Graph_Generation_by_Self-Supervised_Learning_WACV_2023_paper.html"&gt;Improving Predicate Representation in Scene Graph Generation by Self-Supervised Learning&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;S. Hasegawa&lt;/strong&gt;, M. Hiromoto, A. Nakagawa, Y. Umeda&lt;br&gt;
  In IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2023&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Non-peer reviewed&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://onepetro.org/SPEADIP/proceedings-abstract/20ADIP/3-20ADIP/D031S080R004/452202"&gt;Facilitating the Identification of the Nannofossil Species in Cretaceous of Abu Dhabi Using Artificial Intelligence&lt;/a&gt;&lt;br&gt;
  H. Tamamura, M. Yamanaka, S. Chiyonobu, G. Yamada, &lt;strong&gt;S. Hasegawa&lt;/strong&gt;, Y. Totake, T. Nanjo&lt;br&gt;
  In Abu Dhabi International Petroleum Exhibition &amp;amp; Conference, 2020&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.jstage.jst.go.jp/article/jsapmeeting/2017.2/0/2017.2_848/_article/-char/ja/"&gt;Analysis of light absorption characteristics in very-thin single-crystalline silicon solar cells with photonic crystals&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;S. Hasegawa&lt;/strong&gt;, K. Ishizaki, Y. Tanaka, and S. Noda&lt;br&gt;
  In 78th Japan Society of Applied Physics (JSAP) Autumn Meeting, 2017&lt;/li&gt;
&lt;li&gt;&lt;a href="https://confit.atlas.jp/guide/event/jsap2016s/subject/22a-S621-3/advanced"&gt;Numerical analysis of μc-Si solar cells with photonic crystals formed on top surface -Introduction of asymmetric structure-&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;S. Hasegawa&lt;/strong&gt;, K. Ishizaki, Y. Tanaka, A. Motohira, Y. Kawamoto, M. De Zoysa, S. Fujita, and S. Noda&lt;br&gt;
  In 63th Japan Society of Applied Physics (JSAP) Spring Meeting, 2016&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Invited Lectures and Talks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.slideshare.net/SouHasegawa/generative-adversarial-networks-186237021"&gt;Basics and Application of Generative Adversarial Network&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;S. Hasegawa&lt;/strong&gt;&lt;br&gt;
  Invited lecture at Tohoku University, 2019&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.slideshare.net/SouHasegawa/ss-189697036"&gt;About line art colorization&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;S. Hasegawa&lt;/strong&gt;&lt;br&gt;
  Talk at &lt;a href="https://connpass.com/event/149734/"&gt;創作＋機械学習LT会&lt;/a&gt;, 2019&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Education&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Apr 2016 ~ Mar 2018: MEng in Electronics at Kyoto University&lt;ul&gt;
&lt;li&gt;My research was focused on Si/u-Si solar cell incorporating photonic crystals. I had designed the structures of solar cells with numerical simulation and heuristic algorithms (e.g. genetics algorithm and machine learning). Also, I manufactured the solar cells based on my design&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Apr 2012 ~ Mar 2016: BEng in Electrical and Electronic Engineering at Kyoto University&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Work Experience&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Oct 2022 ~ : machine learning researcher at Fujitsu Research of America&lt;ul&gt;
&lt;li&gt;My research is focused on AutoML&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Aug 2021 ~ Sep 2022: machine learning researcher at Fujitsu Research&lt;ul&gt;
&lt;li&gt;My research was focused on scene graph generation, self-supervised learning, and generative models&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sep 2019 - Jan 2020: research assistant at SYMBOL&lt;ul&gt;
&lt;li&gt;My work was focused on developing deep learning-based solutions to extract essential information from 3D data (point cloud, mesh, multi-view)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Apr 2018 ~ Jul 2021: software engineer at Fujitsu&lt;ul&gt;
&lt;li&gt;My work was focused on providing deep learning-based solutions with the customers and developing software products using speech processing and computer vision technologies&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Personal Projects&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/SerialLain3170/Colorization"&gt;Automatic line art colorization&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;Various Implementions of line art colorization with hints and without hints&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/SerialLain3170/AwesomeAnimeResearch"&gt;AwesomeAnimeResearch&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;A massive collection of machine learning papers and projects related to anime&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/SerialLain3170/KawaiiGenerator"&gt;KawaiiGenerator&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;Various implementations of deep learning tasks related to anime&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Skills&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Computer Science&lt;ul&gt;
&lt;li&gt;Machine Learning, Deep learning&lt;/li&gt;
&lt;li&gt;Signal Processing: Computer vision, Speech processing&lt;/li&gt;
&lt;li&gt;Low-level Programming: Emulators of NES, GB, CGB&lt;/li&gt;
&lt;li&gt;Programming Language: Python, Go, C&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Software Development&lt;ul&gt;
&lt;li&gt;Team development&lt;/li&gt;
&lt;li&gt;OS: Linux(Ubuntu, CentOS), MacOS&lt;/li&gt;
&lt;li&gt;Container: Docker, Docker-compose, Kubernetes&lt;/li&gt;
&lt;li&gt;Version management: Git, Gitlab, Github&lt;/li&gt;
&lt;li&gt;DB: MySQL, PostgreSQL, MongoDB&lt;/li&gt;
&lt;li&gt;Others: Nodejs, gRPC, Nginx, RabbitMQ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Language&lt;ul&gt;
&lt;li&gt;Japanese: native&lt;/li&gt;
&lt;li&gt;English: advanced (TOEFL: 96, IELTS: 7.0, GRE: V153/Q170/AW3.5)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Interests&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;All the computer science fields&lt;/li&gt;
&lt;li&gt;Visual arts: watching all kinds of arts, visiting art museums, and learning art history&lt;/li&gt;
&lt;li&gt;Travel and enjoy nature&lt;/li&gt;
&lt;li&gt;Books: computer science, science (biology, geology), art, history, finance&lt;/li&gt;
&lt;/ul&gt;</content><category term="about"></category><category term="about"></category></entry></feed>